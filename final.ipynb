{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db03e84c",
   "metadata": {},
   "source": [
    "# ðŸ“Œ 2. rag/chunker.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "441db52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=40,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\"]\n",
    ")\n",
    "\n",
    "\n",
    "def chunk_field(text: str) -> list:\n",
    "    if not text:\n",
    "        return []\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "\n",
    "def build_corpus(patient: dict) -> list:\n",
    "    docs = []\n",
    "\n",
    "    summary_text = (\n",
    "        f\"Name: {patient.get('name')}. \"\n",
    "        f\"Age/Sex: {patient.get('age')}/{patient.get('sex')}. \"\n",
    "        f\"Conditions: {', '.join(patient.get('conditions', []))}.\"\n",
    "    )\n",
    "\n",
    "    for chunk in chunk_field(summary_text):\n",
    "        docs.append({\n",
    "            \"text\": chunk,\n",
    "            \"source\": \"summary\",\n",
    "            \"meta\": {\"patient_id\": patient[\"id\"]}\n",
    "        })\n",
    "\n",
    "    for chunk in chunk_field(patient.get(\"care_plan\", \"\")):\n",
    "        docs.append({\n",
    "            \"text\": chunk,\n",
    "            \"source\": \"care_plan\",\n",
    "            \"meta\": {\"patient_id\": patient[\"id\"]}\n",
    "        })\n",
    "\n",
    "    for event in patient.get(\"timeline\", []):\n",
    "        txt = f\"{event['date']}: {event['text']}\"\n",
    "        for chunk in chunk_field(txt):\n",
    "            docs.append({\n",
    "                \"text\": chunk,\n",
    "                \"source\": \"timeline\",\n",
    "                \"meta\": {\"patient_id\": patient[\"id\"], \"date\": event[\"date\"]}\n",
    "            })\n",
    "\n",
    "    for chunk in chunk_field(patient.get(\"notes\", \"\")):\n",
    "        docs.append({\n",
    "            \"text\": chunk,\n",
    "            \"source\": \"notes\",\n",
    "            \"meta\": {\"patient_id\": patient[\"id\"]}\n",
    "        })\n",
    "\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98510c2b",
   "metadata": {},
   "source": [
    "# ðŸ“Œ 3. rag/embedder.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cafe21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "_cache_dir = Path(\".cache\")\n",
    "_cache_dir.mkdir(exist_ok=True)\n",
    "\n",
    "_model = None\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    global _model\n",
    "    if _model is None:\n",
    "        _model = SentenceTransformer(MODEL_NAME)\n",
    "    return _model\n",
    "\n",
    "\n",
    "def embed_texts(texts, patient_id: str):\n",
    "    cache_e = _cache_dir / f\"{patient_id}_embeddings.npy\"\n",
    "    cache_meta = _cache_dir / f\"{patient_id}_meta.json\"\n",
    "\n",
    "    if cache_e.exists() and cache_meta.exists():\n",
    "        meta = json.loads(cache_meta.read_text())\n",
    "        if meta.get(\"n_texts\") == len(texts):\n",
    "            return np.load(cache_e)\n",
    "\n",
    "    model = get_model()\n",
    "    embs = model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "    np.save(cache_e, embs)\n",
    "    cache_meta.write_text(json.dumps({\"n_texts\": len(texts)}))\n",
    "\n",
    "    return embs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e150485",
   "metadata": {},
   "source": [
    "# ðŸ“Œ 4. rag/retriever.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30260986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def retrieve(\n",
    "    query_vector: np.ndarray,\n",
    "    docs: List[Dict],\n",
    "    doc_vectors: np.ndarray,\n",
    "    top_k: int = 3,\n",
    "    filter_source: str = None\n",
    ") -> List[Tuple[Dict, float]]:\n",
    "    if filter_source:\n",
    "        filtered_docs = []\n",
    "        filtered_vectors = []\n",
    "        for i, d in enumerate(docs):\n",
    "            if d[\"source\"] == filter_source:\n",
    "                filtered_docs.append(d)\n",
    "                filtered_vectors.append(doc_vectors[i])\n",
    "        if filtered_docs:\n",
    "            docs = filtered_docs\n",
    "            doc_vectors = np.array(filtered_vectors)\n",
    "\n",
    "    scores = cosine_similarity([query_vector], doc_vectors)[0]\n",
    "    top_idx = scores.argsort()[-top_k:][::-1]\n",
    "\n",
    "    return [(docs[i], float(scores[i])) for i in top_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e12c6a7",
   "metadata": {},
   "source": [
    "# ðŸ“Œ 5. rag/router.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "815878d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def route_question(question: str) -> str:\n",
    "    q = question.lower()\n",
    "\n",
    "    if re.search(r\"\\b(when|date|recent|time|happened|follow|appointment)\\b\", q):\n",
    "        return \"timeline\"\n",
    "\n",
    "    if re.search(r\"\\b(plan|med|dose|treatment|instruction)\\b\", q):\n",
    "        return \"care_plan\"\n",
    "\n",
    "    if re.search(r\"\\b(notes|symptom|issue|problem|feeling|status)\\b\", q):\n",
    "        return \"notes\"\n",
    "\n",
    "    if re.search(r\"\\b(condition|summar|overview|profile)\\b\", q):\n",
    "        return \"summary\"\n",
    "\n",
    "    return \"all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ddd4d6",
   "metadata": {},
   "source": [
    "# ðŸ“Œ 6. rag/answer_generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2479f172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "from huggingface_hub import InferenceClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HF_API_KEY = os.environ.get(\"HF_API_KEY\")\n",
    "HF_MODEL = os.environ.get(\"HF_MODEL\", \"openai/gpt-oss-20b\")\n",
    "\n",
    "client = None\n",
    "if HF_API_KEY:\n",
    "    try:\n",
    "        client = InferenceClient(HF_MODEL, token=HF_API_KEY)\n",
    "    except Exception:\n",
    "        client = None\n",
    "\n",
    "\n",
    "def build_prompt(question: str, retrieved_chunks: List[Tuple[Dict, float]]) -> str:\n",
    "    system = (\n",
    "        \"You are a clinical assistant. Answer ONLY using the patient context provided. \"\n",
    "        \"If information is missing, say 'Information not found in patient record.' \"\n",
    "        \"Keep answers concise (1â€“3 sentences).\"\n",
    "    )\n",
    "\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[{doc['source']}] {doc['text']}\"\n",
    "        for doc, _ in retrieved_chunks\n",
    "    ) if retrieved_chunks else \"No context available.\"\n",
    "\n",
    "    return (\n",
    "        f\"{system}\\n\\n\"\n",
    "        f\"Patient context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "\n",
    "def call_hf_llm(prompt: str) -> str:\n",
    "    if not client:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        output = client.text_generation(prompt, max_new_tokens=200, temperature=0.1)\n",
    "        return output.strip()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def fallback_answer(question: str, retrieved_chunks, patient):\n",
    "    q = question.lower()\n",
    "\n",
    "    if \"condition\" in q or \"summar\" in q:\n",
    "        conds = \", \".join(patient.get(\"conditions\", []))\n",
    "        return f\"Conditions: {conds}\" if conds else \"Information not found in patient record.\"\n",
    "\n",
    "    if \"when\" in q or \"review\" in q or \"date\" in q:\n",
    "        for doc, _ in retrieved_chunks:\n",
    "            date = doc[\"meta\"].get(\"date\")\n",
    "            if date:\n",
    "                return f\"Relevant timeline date: {date}. {doc['text']}\"\n",
    "        return \"Information not found in patient record.\"\n",
    "\n",
    "    if \"recent\" in q or \"happened\" in q:\n",
    "        if retrieved_chunks:\n",
    "            doc, _ = retrieved_chunks[0]\n",
    "            return f\"Most recent entry: {doc['text']}\"\n",
    "        return \"Information not found in patient record.\"\n",
    "\n",
    "    if retrieved_chunks:\n",
    "        return \" \".join(doc[\"text\"] for doc, _ in retrieved_chunks)\n",
    "\n",
    "    return \"No relevant information found.\"\n",
    "\n",
    "\n",
    "def generate_answer(question, retrieved_chunks, route, patient_meta):\n",
    "    prompt = build_prompt(question, retrieved_chunks)\n",
    "    llm_out = call_hf_llm(prompt)\n",
    "\n",
    "    if llm_out:\n",
    "        return llm_out\n",
    "\n",
    "    return fallback_answer(question, retrieved_chunks, patient_meta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be86e64c",
   "metadata": {},
   "source": [
    "# ðŸ“Œ 1. chatbot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20af03af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# from rag.chunker import build_corpus\n",
    "# from rag.embedder import embed_texts, get_model\n",
    "# from rag.retriever import retrieve\n",
    "# from rag.router import route_question\n",
    "# from rag.answer_generator import generate_answer\n",
    "\n",
    "\n",
    "def load_patient(patient_id: str):\n",
    "    pfile = Path(\"patients\") / f\"{patient_id}.json\"\n",
    "    if not pfile.exists():\n",
    "        raise FileNotFoundError(f\"Patient file not found: {pfile}\")\n",
    "    return json.loads(pfile.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "\n",
    "\n",
    "def prepare_patient(patient: dict):\n",
    "    pid = patient[\"id\"]\n",
    "\n",
    "    docs = build_corpus(patient)\n",
    "    texts = [d[\"text\"] for d in docs]\n",
    "    vectors = embed_texts(texts, pid)\n",
    "\n",
    "    return docs, vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f53f6d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = \"P001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8da13ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'P001',\n",
       " 'name': 'Aarti Sharma',\n",
       " 'age': 32,\n",
       " 'sex': 'F',\n",
       " 'conditions': ['Gestational Diabetes', 'Iron-deficiency Anemia'],\n",
       " 'care_plan': '12-week antenatal care: weekly fasting and post-prandial glucose monitoring; start oral iron supplement (60mg elemental) daily; nutrition counseling and follow-up. Next review scheduled on 2025-12-10 for glycemic control assessment.',\n",
       " 'timeline': [{'date': '2025-11-20',\n",
       "   'text': 'First antenatal visit; baseline labs collected.'},\n",
       "  {'date': '2025-11-25',\n",
       "   'text': 'Nutrition counselling given; advised diet changes.'},\n",
       "  {'date': '2025-12-01',\n",
       "   'text': 'Fasting glucose elevated; advised stricter monitoring.'}],\n",
       " 'notes': 'Patient reports fatigue and mild dizziness. Adherent to diet but occasional missed doses of iron. Will monitor BP and glucose at home.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient = load_patient(patient_id)\n",
    "patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ddcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'text': 'Name: Aarti Sharma. Age/Sex: 32/F. Conditions: Gestational Diabetes, Iron-deficiency Anemia.',\n",
       "   'source': 'summary',\n",
       "   'meta': {'patient_id': 'P001'}},\n",
       "  {'text': '12-week antenatal care: weekly fasting and post-prandial glucose monitoring; start oral iron supplement (60mg elemental) daily; nutrition counseling and follow-up. Next review scheduled on 2025-12-10 for glycemic control assessment.',\n",
       "   'source': 'care_plan',\n",
       "   'meta': {'patient_id': 'P001'}},\n",
       "  {'text': '2025-11-20: First antenatal visit; baseline labs collected.',\n",
       "   'source': 'timeline',\n",
       "   'meta': {'patient_id': 'P001', 'date': '2025-11-20'}},\n",
       "  {'text': '2025-11-25: Nutrition counselling given; advised diet changes.',\n",
       "   'source': 'timeline',\n",
       "   'meta': {'patient_id': 'P001', 'date': '2025-11-25'}},\n",
       "  {'text': '2025-12-01: Fasting glucose elevated; advised stricter monitoring.',\n",
       "   'source': 'timeline',\n",
       "   'meta': {'patient_id': 'P001', 'date': '2025-12-01'}},\n",
       "  {'text': 'Patient reports fatigue and mild dizziness. Adherent to diet but occasional missed doses of iron. Will monitor BP and glucose at home.',\n",
       "   'source': 'notes',\n",
       "   'meta': {'patient_id': 'P001'}}],\n",
       " array([[-0.02067659,  0.02474545, -0.03729772, ..., -0.07349352,\n",
       "         -0.10107341, -0.01380576],\n",
       "        [-0.02563836,  0.01712662, -0.07587159, ..., -0.12865306,\n",
       "         -0.04559799, -0.05732273],\n",
       "        [-0.01326094, -0.0077975 ,  0.04383273, ..., -0.02026877,\n",
       "         -0.00561489, -0.03505623],\n",
       "        [-0.07584479, -0.0057177 ,  0.04994728, ..., -0.05371577,\n",
       "         -0.04440679, -0.0256628 ],\n",
       "        [-0.06893238,  0.06480008, -0.02442718, ..., -0.04199512,\n",
       "         -0.05003566, -0.01458098],\n",
       "        [-0.00237571, -0.02712578, -0.06952541, ..., -0.06109888,\n",
       "         -0.05446673, -0.00885128]], shape=(6, 384), dtype=float32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs, doc_vectors = prepare_patient(patient)\n",
    "docs, doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = get_model()\n",
    "while True:\n",
    "    query = input(\"Q> \").strip()\n",
    "    if query.lower() in (\"exit\", \"quit\"):\n",
    "        print(\"bye\")\n",
    "        break\n",
    "\n",
    "    route = route_question(query)\n",
    "    print(route)\n",
    "    query_vector = embedder.encode([query])[0]\n",
    "    print(query_vector)\n",
    "\n",
    "    retrieved = retrieve(\n",
    "        query_vector=query_vector,\n",
    "        docs=docs,\n",
    "        doc_vectors=doc_vectors,\n",
    "        top_k=3,\n",
    "        filter_source=route\n",
    "    )\n",
    "    print(retrieved)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    answer = generate_answer(\n",
    "        question=query,\n",
    "        retrieved_chunks=retrieved,\n",
    "        route=route,\n",
    "        patient_meta=patient\n",
    "    )\n",
    "    print(answer)\n",
    "\n",
    "    print(\"\\n--- Retrieved Context ---\")\n",
    "    for doc, score in retrieved:\n",
    "        print(f\"[{doc['source']}] score={score:.4f} | {doc['text']}\")\n",
    "\n",
    "    print(\"\\n--- Answer ---\")\n",
    "    print(answer)\n",
    "    print(\"\\n-------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10341ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def interactive_chat(patient_id: str):\n",
    "    patient = load_patient(patient_id)\n",
    "    docs, doc_vectors = prepare_patient(patient)\n",
    "    embedder = get_model()\n",
    "\n",
    "    print(f\"\\nLoaded patient {patient_id}: {patient.get('name')}\")\n",
    "    print(f\"Number of chunks: {len(docs)}\")\n",
    "    print(\"Type 'exit' or 'quit' to stop.\\n\")\n",
    "\n",
    "    while True:\n",
    "        query = input(\"Q> \").strip()\n",
    "        if query.lower() in (\"exit\", \"quit\"):\n",
    "            print(\"bye\")\n",
    "            break\n",
    "\n",
    "        route = route_question(query)\n",
    "        query_vector = embedder.encode([query])[0]\n",
    "\n",
    "        retrieved = retrieve(\n",
    "            query_vector=query_vector,\n",
    "            docs=docs,\n",
    "            doc_vectors=doc_vectors,\n",
    "            top_k=3,\n",
    "            filter_source=route\n",
    "        )\n",
    "\n",
    "\n",
    "        answer = generate_answer(\n",
    "            question=query,\n",
    "            retrieved_chunks=retrieved,\n",
    "            route=route,\n",
    "            patient_meta=patient\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Retrieved Context ---\")\n",
    "        for doc, score in retrieved:\n",
    "            print(f\"[{doc['source']}] score={score:.4f} | {doc['text']}\")\n",
    "\n",
    "        print(\"\\n--- Answer ---\")\n",
    "        print(answer)\n",
    "        print(\"\\n-------------------------\\n\")\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\"--patient\", required=True,\n",
    "#                         help=\"Patient ID (P001, P002...)\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     pid = args.patient.upper()\n",
    "#     interactive_chat(pid)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751c27f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
